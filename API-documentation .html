<!DOCTYPE html>

<html>
<head>
  <meta charset="utf-8">
  <link href="https://markable.in/static/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="https://markable.in//static/css/style.css" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="https://markable.in/static/editor/css/view_file.css">
  <link rel="stylesheet" type="text/css" href="https://markable.in/static/css/code.css">
</head>
<body>
  <div class="container">
    <div id="content">
      <h2 id="api-tutorial-for-waste-image-classification">API tutorial for waste image classification</h2>
<p>This tutorial is a documentation of a keras API, we will be presenting a method developed by Harold Waste staff in order to deploy a REST API for image classification.</p>
<p>The examples covered in this post will serve as a starting point for building your own deep learning APIs, further extension of the code is possible based on your needs and on how scalable your API endpoint needs to be.</p>
<p>Specifically, we will be discovering : 
- how to load a keras model into memory so it can be efficiently used for inference </p>
<ul>
<li>
<p>How to use the Flask web framework to create an endpoint for our API</p>
</li>
<li>
<p>How to make predictions using our model, JSON-ify them, and return the results to the client</p>
</li>
<li>
<p>How to call our Keras REST API using both terminal and postman</p>
</li>
</ul>
<p>By the end of this tutorial you'll have a good understanding of the components that go into a creating Keras REST API.</p>
<p>Feel free to use the code presented in this guide as a starting point for your own deep learning <a href="https://www.restapitutorial.com/">REST API</a>.</p>
<p>Note: The method covered here is intended to be instructional. It is not meant to be production-level and capable of scaling under heavy load. It was developed on top of intern models we made.</p>
<h2 id="configuration-of-the-development-environment">Configuration of the development environment</h2>
<p>We'll suppose that keras is alread configured and installed on your machine. If not, please make sure you install keras using the official instructions.</p>
<p>We'll be making the assumption that Keras is already configured and installed on your machine. If not, please ensure you install Keras using the <a href="https://keras.io/#installation">official install instructions</a>.</p>
<p>From there, we'll need to install Flask (and its associated dependencies), a Python web framework, so we can build our API endpoint. We'll also need requests so we can consume our API as well.</p>
<p>From there, we can easily install <a href="http://flask.pocoo.org/">Flask</a> (and its associated dependecies), a web framework designed for building API endpoint. We'll also need <a href="http://docs.python-requests.org/en/master/">requests</a> so we can consume our API as well.</p>
<p>We'll be using the pip command for installing the needed packages.</p>
<pre><code> $ pip install -r requirements.txt

</code></pre>

<p>I strongly recommend that you work within a virtual environment, it helps keep your computer away of incompatible versions of packages </p>
<p>Here is a great <a href="https://docs.python-guide.org/dev/virtualenvs/">tutorial</a> that makes a brief and concise introduction to <a href="https://docs.python-guide.org/dev/virtualenvs/">virtual environnement</a> alternative.  </p>
<p>Essentially these instruction will be helpful :</p>
<p>To install (make sure virtualenv is already installed)</p>
<pre><code>$ pip install virtualenvwrapper
$ export WORKON_HOME=~/Envs
$ source /usr/local/bin/virtualenvwrapper.sh

</code></pre>

<p>1- Create a virtual environment:</p>
<pre><code>$ mkvirtualenv my_project
</code></pre>

<p>This creates the my_project folder inside ~/Envs.</p>
<p>2- Activate the virtual environement</p>
<pre><code>$ workon my_project
</code></pre>

<p>3-Deactivating is also possible:</p>
<pre><code>$ deactivate
</code></pre>

<p>4- delete is also allowed </p>
<pre><code>$ rmvirtualenv venv
</code></pre>

<h2 id="building-harold-keras-rest-api">Building Harold Keras REST API</h2>
<p>Our Keras REST API is self-contained in a single file named app.py. We kept the installation in a single file as a manner of simplicity.</p>
<p>Inside app.py you'll find functions necessary for our classification task.</p>
<p>Our three main function are namely:</p>
<ul>
<li>upload-model-weights: Used to load our trained Keras model (architecture &amp; weights), prepare it for inference.</li>
<li>process_data: This function preprocesses an input image prior to passing it through our network for prediction</li>
<li>predict: Our API endopoint which will classify the incoming data from the request and return the results to the clients as a json file.</li>
</ul>
<p>The <a href="https://github.com/WassimAbida/Harold-API">full code</a> to this tutorial can be found <a href="https://github.com/WassimAbida/Harold-API">here</a>.</p>
<p>Our API structure has access to a certain number of pre-trained deep learning models on our internal data sets.</p>
<p>Our first code handles importing our required packages, initializing the Flask application and displaying available deep learning models for testing.</p>
<pre><code>import os
from flask import Flask, render_template, request, jsonify
import numpy as np
from PIL import Image
import codecs, json
import tensorflow as tf
from keras.models import model_from_json

app = Flask(__name__)



</code></pre>

<p>Thus our models are structered in folders and path to models is given as input to our API for constructing its own dictionnary of models.</p>
<pre><code>Path_to_models = '/home/Desktop/name_project/src/models/'
dictio_models_creator(Path_to_models)
</code></pre>

<p>The function dictio-models-creator creates a dictionnary containing names of all models availables along with paths to their weights and architecture, </p>
<p><img src="https://raw.githubusercontent.com/WassimAbida/Harold-API/master/images_to_markdown/ssssd.jpg" width="600"></p>
<p>Output:</p>
<pre><code>
{
'convAE': {'classifier_model_ConvAE.h5','classifier_model_ConvAE.json'},

'denoiseAE': {'classifier_model_DenoiseAE.h5','classifier_model_DenoiseAE.json'},

'vgg16': {'model_vgg16_transfer_learning.h5','model_vgg16_transfer_learning.json'}
 }

</code></pre>

<p>After Manually choising the model to adopt, we can to move to uploading the model architecture and weights.</p>
<p>From there we define the upload-model-weights function:</p>
<pre><code class="css">

def upload_model_weights(path_to_model, path_to_weights):
    global model

    &quot;&quot;&quot; This function takes as input the path to the model architecture      and weights&quot;&quot;&quot;
# load json and create model
    my_json_file = open(path_to_model, 'r')
    loaded_model_json = my_json_file.read()
    my_json_file.close()
    model = model_from_json(loaded_model_json)
# load weights into new model
    model.load_weights(path_to_weights)

    print(&quot;Loaded  model and weights from disk&quot;)

    return model


</code></pre>

<p>As the name suggests, this method is responsible for instantiating our architecture and loading our weights from disk.</p>
<p>Before we can perfrom classfication over data coming from our client, we firts need to prepare and preprocess the data: </p>
<p>Processing data goes through 2 steps : 
- converting images to numpy array 
- scaling and normalizing data</p>
<pre><code>def data_to_images(APP_ROOT):
    &quot;&quot;&quot; This function performs a request for reading data from a repository and then transform the images into numpy arrays&quot;&quot;&quot;

    x_data = []
    target = os.path.join(APP_ROOT, 'images')
    print(&quot;our target = &quot;, target)

    if not os.path.isdir(target):
        os.mkdir(target)
    print(len(request.files.getlist(&quot;my_file&quot;)))

    for file in request.files.getlist(&quot;my_file&quot;):
        print(&quot; file object  = &quot;, file)
        filename = file.filename
        destination = &quot;/&quot;.join([target, filename])
        print(&quot;path to image =&quot;, destination)

        file.save(destination)
        # taking inputs and saving 'em into tensor form
        img = Image.open(destination)

        # shape should be adjusted to model requieries  default (128,128)
        img = img.resize((128, 128), Image.ANTIALIAS)
        img = np.asarray(img)
        x_data.append(img)
        os.remove(destination)
    # my data tensor
    x_data = np.array(x_data)
    return x_data



def process_data(APP_ROOT):
    &quot;&quot;&quot; this function processes data and make them centered,

    it first uses the pre-defined function data_to_images to fulfill a request for reading images &quot;&quot;&quot;

    x_data = data_to_images(APP_ROOT)
    x_data = (x_data - np.min(x_data, 0)) / (np.max(x_data, 0) + 0.0001)
    return x_data

</code></pre>

<p>This function perform a request for reading an input, save the input in a local directory, then convert it into a more suitable form for our model.
This function
- Performs a request for reading an input
- Saves the input in a local directory
- Convert it into a more suitable form for our model, ideally a numpy array of shape 128x128 pixels for vgg16 for example.
- Preprocesses the array via mean substraction and scaling </p>
<p>We are now ready to define the predict function — this method processes any requests to the /predict endpoint:</p>
<pre><code>@app.route(&quot;/predict&quot;, methods = ['POST'])
def predict():

# processing data
    if request.method == &quot;POST&quot;:
        x_data= process_data()
        print(&quot; input data shape  &quot;, x_data.shape)
    # classify the input image and then initialize the 
    # list of predictions to return to the client

        class_predicted=[]
        for i in range(len(x_data)):
        # reshaping every input in tensor (1,width,height,channels)
            image = x_data[i].reshape(1, x_data[i].shape[0], x_data[i].shape[1], x_data[i].shape[2])
            with graph.as_default():
                prediction = predict_function(model, image,dictionnary)
            print('my predictions : ', prediction, '\n')
            class_predicted.append(prediction[0][0])

        data = {&quot;success&quot;: False}
        data[&quot;predictions&quot;] = []
    # loop over the results and add them to the list of
    # returned predictions

        for (label, prob) in class_predicted:
            r = {&quot;label&quot;: label, &quot;probability&quot;: float(prob)}
            data[&quot;predictions&quot;].append(r)

    # indicate that the request was a success
        data[&quot;success&quot;] = True

    return jsonify(data)
</code></pre>

<p>The data dictionary is used to store any data that we want to return to the client. 
In our case, it includes a boolean used to indicate if prediction was successful or not add to results of predictions made on the incoming data.</p>
<p>Before accepting the incoming data we check whether : 
- The request method is POST, this litterarely enables us to send arbitrary data to endpoint such as images, JSON...</p>
<p>We might also wanna check if an image has been passed into the files attribute during the POST</p>
<pre><code>if flask.request.files.get(&quot;image&quot;):
    {...
    }

</code></pre>

<p>files code and either parse the raw input data yourself or utilize request.get_json() to automatically parse the input data to a Python dictionary/object.</p>
<p>We then take the incoming data and:
- Call out the function process_data which will read input in PIL format
- Preprocess it 
- Pass it through our pre-trained network
- Loop predictions over the input data and add them individually to the data["predictions"] list
- Return the response to the client in JSON format </p>
<p>For a general context, when working with non-image data, you might remove the request.
If you're working with non-image data you should remove the request.files code and either parse the raw input data yourself or utilize request.get_json() to automatically parse the input data to a Python dictionary/object. </p>
<p>Additionally, 
Additionally, consider giving <a href="https://scotch.io/bar-talk/processing-incoming-request-data-in-flask">following tutorial</a> a read which discusses the fundamentals of Flask's request object.</p>
<p>Our final step now is to launch our service: </p>
<pre><code>
if __name__ == &quot;__main__&quot;:
    upload_model_weights(path_to_model=path_to_model, path_to_weights=path_to_weights)
    app.run(port=4555,debug=True)

</code></pre>

<p>First we call the function upload-model-weights which loades our keras model from disk and then we call our app to run.</p>
<p>This is a basic representaion of what logically happens but I have omit the part where we give user the possibility to choose the model he wants.</p>
<p>A full code is cited here:</p>
<pre><code>
if __name__ == &quot;__main__&quot;:

    Path_to_models = /Path/to/your/models/

    Dictio_models = dictio_models_creator(Path_to_models)
    print(&quot;deep learning models available&quot;, list(Dictio_models.keys()))

    model_choice = input('choose a model for the classification task : ')
    paths = list(Dictio_models[model_choice])
    print('list of paths to weights and architecture',paths)
    # we want to ensure that path_to_weights take the data from file.h5

    if (paths[1]).endswith('h5'):

        path_to_weights, path_to_model = Path_to_models + model_choice + '/' + paths[
            1], Path_to_models + model_choice + '/' + paths[0]
    else:
        path_to_weights, path_to_model = Path_to_models + model_choice + '/' + paths[
            0], Path_to_models + model_choice + '/' + paths[1]

    upload_model_weights(path_to_model=path_to_model, path_to_weights=path_to_weights)
    app.run(port=4555,debug=True)

</code></pre>

<h2 id="starting-your-keras-rest-api">Starting your Keras Rest API</h2>
<p>Starting the Keras REST API service is easy.</p>
<p>Open up a terminal, go to the directory including your python file app.py and execute:</p>
<pre><code>$ python3 app.py
Using TensorFlow backend.
deep learning models available ['denoiseAE', 'vgg16', 'convAE']
Choose a model for the classification task : %%

...

Loaded  model and weights from disk
 * Serving Flask app &quot;app&quot; (lazy loading)
 * Debug mode: on
 * Running on http://127.0.0.1:4555/ (Press CTRL+C to quit)


</code></pre>

<h3 id="possible-issue-when-working-on-terminal-and-pycharm-at-the-same-time">Possible issue when working on terminal and PyCharm at the same time</h3>
<p>You might notice that there is a bug saying </p>
<pre><code>ImportError: No module named 'src'
</code></pre>

<p>You can solve this issue by logging into the app code and removing the word src from the import path.
 check for app.py and prediction.py</p>
<pre><code>from src.service.load_all_models import load_all_models, upload_model_weights
from src.service.prediction import predict_function
</code></pre>

<p>to </p>
<pre><code>
from service.load_all_models import load_all_models, upload_model_weights
from service.prediction import predict_function
</code></pre>

<p>As you can see from the output, our model is loaded first — after which we can start our Flask server.
You can now access the server via http://127.0.0.1:4555/</p>
<p>For me, I prefer working with PyCharm, he gives you direct access to your code and provides also the debugging task wich helps detect any kind of problems and performs checkpoints.</p>
<p><img src="https://raw.githubusercontent.com/WassimAbida/Harold-API/master/images_to_markdown/pycharm.png" width="600"></p>
<p>For running the API, you can simply press the button Run app.py 
and then our instance is activated.</p>
<p>For fulfilling the task of uploading data, we omit the html way which is not very efficent and prefer the application postman which helps us interact with our API through a dynamic interface.</p>
<p>Postman help achieve simple tasks as POST and GET through two or three clicks </p>
<p><img src="https://raw.githubusercontent.com/WassimAbida/Harold-API/master/images_to_markdown/post.png" width="600"></p>
<p>As we can see in the image above, POSTMAN give you a large set of possible action to perform within an API.</p>
<p>For instance, in our API we are intrested in performing a POST request to upload our data images, so we cite here the link to our local server " http://127.0.0.1:4555/predict ", and we press on the label Body, we'll have then access to our data by clicking on "Choose Files" </p>
<p><img src="https://raw.githubusercontent.com/WassimAbida/Harold-API/master/images_to_markdown/send.png" width="600"></p>
<p>You simply then choose your images data and press "Send", Output will then be printed within POSTMAN as a json file </p>
<p><img src="https://raw.githubusercontent.com/WassimAbida/Harold-API/master/images_to_markdown/predict.png" width="600"></p>
<p>And here we come to our main goal which is visualizing the predictions over a set of input images.
Output is in format json file containing data of different types and collected into a dictionnary.</p>
<p>I hope this documentation helps you to understand the mechanism developed to build this classification API.</p>
<p>Don't hesitate to comment or to contact me in case of a problem or a suggestion to make.</p>
<p>You can find here link to <a href="wassim.abida14@gmail.com">my_mail</a>.</p>
<p>The code is been saved as a github project in this <a href="https://github.com/WassimAbida/Harold-API">url</a>.</p>
<p>You can find a <a href="https://help.github.com/articles/adding-an-existing-project-to-github-using-the-command-line/">quick and useful tutorial</a> for github project management. </p>
    </div>
  </div>
</body>
</html>